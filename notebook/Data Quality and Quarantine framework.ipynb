{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a87bc74e-427b-409e-82c0-a3d35c486ebb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Quality and Quarantine Framework : Validation Engine Logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af89f989-189b-4cb2-91b9-74f7f3c6c748",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Data source configuration (Catalog, Schema, Tables, PK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e87a5f4-668e-4297-84f6-1e760eda3169",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql.functions import split, trim, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92f5f4e8-6f1f-4dc7-801e-075c75946488",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Widgets & Setup\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "dbutils.widgets.text(\"catalog\", \"\")\n",
    "dbutils.widgets.text(\"schema\", \"\")\n",
    "dbutils.widgets.text(\"tables\", \"\")\n",
    "dbutils.widgets.text(\"Input_file_location_pk\", \"\")\n",
    "#dbutils.widgets.text(\"pk\", \"\")\n",
    "def _get_widget(name: str) -> str:\n",
    "    return (dbutils.widgets.get(name) or \"\").strip()\n",
    "\n",
    "catalog    = _get_widget(\"catalog\")\n",
    "schema     = _get_widget(\"schema\")\n",
    "tables_raw = _get_widget(\"tables\")\n",
    "input_pk = _get_widget(\"Input_file_location_pk\")\n",
    "\n",
    "file_path = f\"/Volumes/{catalog}/{schema}/{input_pk}\"\n",
    "pk_df = spark.read.text(file_path)\n",
    "\n",
    "if not catalog or not schema:\n",
    "    raise RuntimeError(\"Please fill 'catalog' and 'schema' widgets.\")\n",
    "if not tables_raw:\n",
    "    raise RuntimeError(\"Please fill 'tables' widget.\")\n",
    "# if not pk_raw:\n",
    "#     raise RuntimeError(\"Please fill 'pk' widget.\")\n",
    "\n",
    "meta = f\"{catalog}.{schema}\"\n",
    "\n",
    "import re\n",
    "from typing import Dict, List\n",
    "\n",
    "def normalize_logical_name(table_name: str) -> str:\n",
    "    base = table_name.strip().split(\".\")[-1]\n",
    "    return re.sub(r'^(bronze)_', '', base, flags=re.IGNORECASE)\n",
    "\n",
    "def source_table_name(meta: str, source_hint: str) -> str:\n",
    "    return source_hint if \".\" in source_hint else f\"{meta}.{source_hint}\"\n",
    "\n",
    "def silver_table_name(meta: str, logical_name: str) -> str:\n",
    "    return f\"{meta}.silver_{logical_name}_valid\"\n",
    "\n",
    "def get_pk_dict(pk_df):\n",
    "    parsed_df = (\n",
    "        pk_df\n",
    "        .withColumn(\"table_name\", trim(split(col(\"value\"), \":\")[0]))\n",
    "        .withColumn(\"pk_columns_raw\", trim(split(col(\"value\"), \":\")[1]))\n",
    "        .withColumn(\"primary_keys\", split(col(\"pk_columns_raw\"), \",\"))\n",
    "        .drop(\"value\", \"pk_columns_raw\")\n",
    "    )\n",
    "    parsed_df = parsed_df.withColumn(\n",
    "        \"primary_keys\",\n",
    "        expr(\"transform(primary_keys, x -> trim(x))\")\n",
    "    )\n",
    "    mapping = {\n",
    "        row[\"table_name\"]: row[\"primary_keys\"]\n",
    "        for row in parsed_df.collect()\n",
    "    }\n",
    "    return mapping\n",
    "\n",
    "\n",
    "def get_pk_for_table_from_widget(source_input_name: str, src_cols: List[str]) -> List[str]:\n",
    "    pk_map = get_pk_dict(pk_df)\n",
    "    logical = normalize_logical_name(source_input_name)\n",
    "\n",
    "    for key in (source_input_name, logical):\n",
    "        if key in pk_map:\n",
    "            pk_cols = pk_map[key]\n",
    "            break\n",
    "    else:\n",
    "        raise RuntimeError(f\"PK not defined for table {source_input_name}\")\n",
    "\n",
    "    missing = [c for c in pk_cols if c not in src_cols]\n",
    "    if missing:\n",
    "        raise RuntimeError(f\"Missing PK columns {missing} in {source_input_name}\")\n",
    "\n",
    "    return pk_cols\n",
    "\n",
    "def _fmt_line(label: str, value: str, width: int = 12):\n",
    "    print(f\"{label:<{width}}  : {value}\")\n",
    "\n",
    "tables_list = [t.strip() for t in tables_raw.split(\",\") if t.strip()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4b52d3a-025f-46c4-bfa1-dae9b5b1894f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#Validation Engine Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f15442c3-fa26-4f16-892f-40c44565649b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# FULL DATA QUALITY PIPELINE (FINAL ‚Äì STABLE, METADATA-DRIVEN)\n",
    "# PKs from PK file ONLY\n",
    "# dq_column_group ‚Üí merge logic ONLY\n",
    "# ALL source columns preserved in Silver\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from functools import reduce\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# =============================================================\n",
    "# PIPELINE METADATA\n",
    "# =============================================================\n",
    "PIPELINE_RUN_ID = int(time.time() * 1000)\n",
    "PIPELINE_TS     = datetime.now()\n",
    "\n",
    "# =============================================================\n",
    "# FRAMEWORK TABLES\n",
    "# =============================================================\n",
    "RULES_DEF_TBL    = f\"{meta}.rule_defination\"\n",
    "COLUMN_MAP_TBL   = f\"{meta}.column_map\"\n",
    "COLUMN_GROUP_TBL = f\"{meta}.dq_column_group\"\n",
    "QUARANTINE_TBL   = f\"{meta}.bronze_dq_quarantine\"\n",
    "OBS_TBL          = f\"{meta}.dq_observability_summary\"\n",
    "\n",
    "rules_def  = spark.table(RULES_DEF_TBL).select(\"rule_id\",\"rule_type\",\"threshold\")\n",
    "column_map = spark.table(COLUMN_MAP_TBL)\n",
    "\n",
    "# =============================================================\n",
    "# HELPERS\n",
    "# =============================================================\n",
    "def _fmt_line(label, value, width=22):\n",
    "    print(f\"{label:<{width}} : {value}\")\n",
    "\n",
    "def add_row_hash_and_id(df):\n",
    "    return (\n",
    "        df\n",
    "        .withColumn(\n",
    "            \"_row_hash\",\n",
    "            F.sha2(\n",
    "                F.concat_ws(\n",
    "                    \"||\",\n",
    "                    *[F.coalesce(F.col(c).cast(\"string\"), F.lit(\"‚àÖ\")) for c in df.columns]\n",
    "                ),\n",
    "                256\n",
    "            )\n",
    "        )\n",
    "        .withColumn(\"_row_id\", F.monotonically_increasing_id())\n",
    "    )\n",
    "\n",
    "# =============================================================\n",
    "# CORE VALIDATION\n",
    "# =============================================================\n",
    "def run_validation_for_table(source_input_name: str, run_id: int):\n",
    "\n",
    "    source_table = source_table_name(meta, source_input_name)\n",
    "    logical_name = normalize_logical_name(source_input_name)\n",
    "    silver_table = silver_table_name(meta, logical_name)\n",
    "\n",
    "    df = spark.table(source_table)\n",
    "    base_cols = df.columns\n",
    "\n",
    "    # üîë PKs strictly from PK file\n",
    "    pk_cols = get_pk_for_table_from_widget(source_input_name, base_cols)\n",
    "\n",
    "    df_hashed = add_row_hash_and_id(df)\n",
    "\n",
    "    # =========================================================\n",
    "    # 1Ô∏è‚É£ FULL ROW DUPLICATES ‚Üí QUARANTINE\n",
    "    # =========================================================\n",
    "    dup_failures = (\n",
    "        df_hashed\n",
    "        .withColumn(\n",
    "            \"dup_rank\",\n",
    "            F.row_number().over(\n",
    "                Window.partitionBy(\"_row_hash\").orderBy(\"_row_id\")\n",
    "            )\n",
    "        )\n",
    "        .filter(F.col(\"dup_rank\") > 1)\n",
    "        .select(\n",
    "            \"_row_id\",\n",
    "            F.struct(*base_cols).alias(\"row_struct\"),\n",
    "            F.lit(2).alias(\"failed_rule_id\"),\n",
    "            F.lit(\"full_row_duplicate\").alias(\"failed_rule_type\"),\n",
    "            F.lit(\"Exact duplicate row detected\").alias(\"failure_reason\"),\n",
    "            F.lit(\"_all_columns\").alias(\"column_name\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # =========================================================\n",
    "    # 2Ô∏è‚É£ COLUMN-LEVEL RULE FAILURES\n",
    "    # =========================================================\n",
    "    rule_map = (\n",
    "        column_map\n",
    "        .filter(F.col(\"table_name\") == logical_name)\n",
    "        .withColumn(\n",
    "            \"rule_id\",\n",
    "            F.explode(F.split(F.regexp_replace(\"rule_ids\",\"\\\\s+\",\"\"), \",\"))\n",
    "        )\n",
    "        .withColumn(\"rule_id\", F.col(\"rule_id\").cast(\"int\"))\n",
    "        .join(rules_def, \"rule_id\")\n",
    "        .filter(F.col(\"column_name\").isin(base_cols))\n",
    "    )\n",
    "\n",
    "    def failing_rows_for_rule(df, r):\n",
    "        col   = r[\"column_name\"]\n",
    "        rtype = r[\"rule_type\"]\n",
    "        thr   = (r.get(\"threshold\") or \"\").strip()\n",
    "\n",
    "        if rtype == \"null_check\":\n",
    "            cond = F.col(col).isNull()\n",
    "        elif rtype.startswith(\"regex_\"):\n",
    "            cond = F.col(col).isNotNull() & (~F.col(col).cast(\"string\").rlike(thr))\n",
    "        elif rtype.startswith(\"range_\"):\n",
    "            lo, hi = map(float, thr.split(\"-\"))\n",
    "            cond = (\n",
    "                F.col(col).isNotNull() &\n",
    "                (\n",
    "                    F.col(col).cast(\"double\").isNull() |\n",
    "                    (F.col(col).cast(\"double\") < lo) |\n",
    "                    (F.col(col).cast(\"double\") > hi)\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "        return (\n",
    "            df.filter(cond)\n",
    "              .select(\"_row_id\", F.struct(*base_cols).alias(\"row_struct\"))\n",
    "              .withColumn(\"failed_rule_id\", F.lit(r[\"rule_id\"]))\n",
    "              .withColumn(\"failed_rule_type\", F.lit(rtype))\n",
    "              .withColumn(\"failure_reason\", F.lit(f\"{col} failed {rtype}\"))\n",
    "              .withColumn(\"column_name\", F.lit(col))\n",
    "        )\n",
    "\n",
    "    rule_failures = []\n",
    "    for r in rule_map.collect():\n",
    "        fr = failing_rows_for_rule(df_hashed, r.asDict())\n",
    "        if fr is not None:\n",
    "            rule_failures.append(fr)\n",
    "\n",
    "    all_failures = (\n",
    "        reduce(lambda a,b: a.unionByName(b), [dup_failures] + rule_failures)\n",
    "        if rule_failures else dup_failures\n",
    "    )\n",
    "\n",
    "    invalid_rows = all_failures.select(\"_row_id\").distinct()\n",
    "\n",
    "    # =========================================================\n",
    "    # 3Ô∏è‚É£ QUARANTINE WRITE\n",
    "    # =========================================================\n",
    "    (\n",
    "        all_failures\n",
    "        .withColumn(\"run_id\", F.lit(run_id))\n",
    "        .withColumn(\"source_table\", F.lit(logical_name))\n",
    "        .withColumn(\"invalid_data\", F.to_json(\"row_struct\"))\n",
    "        .withColumn(\"processed_timestamp\", F.current_timestamp())\n",
    "        .withColumnRenamed(\"_row_id\", \"row_id\")\n",
    "        .select(\n",
    "            \"run_id\",\"source_table\",\"invalid_data\",\n",
    "            \"failed_rule_id\",\"failed_rule_type\",\n",
    "            \"failure_reason\",\"processed_timestamp\",\"row_id\"\n",
    "        )\n",
    "        .write.mode(\"append\")\n",
    "        .saveAsTable(QUARANTINE_TBL)\n",
    "    )\n",
    "\n",
    "    # =========================================================\n",
    "    # 4Ô∏è‚É£ CLEAN DATA (PRE-AGG)\n",
    "    # =========================================================\n",
    "    clean_df = (\n",
    "        df_hashed\n",
    "        .join(invalid_rows, \"_row_id\", \"left_anti\")\n",
    "        .drop(\"_row_hash\", \"_row_id\")\n",
    "    )\n",
    "\n",
    "    clean_pre_agg_cnt = clean_df.count()\n",
    "\n",
    "    # =========================================================\n",
    "    # 5Ô∏è‚É£ SILVER ‚Äì MERGE COLUMNS ONLY, KEEP ALL OTHERS\n",
    "    # =========================================================\n",
    "    cg_rows = (\n",
    "        spark.table(COLUMN_GROUP_TBL)\n",
    "        .filter((F.col(\"table_name\") == logical_name) & (F.col(\"active_flag\") == \"Y\"))\n",
    "        .select(\"column_name\",\"merge_strategy\",\"delimiter\")\n",
    "        .collect()\n",
    "    )\n",
    "\n",
    "    merge_cols = [r.column_name for r in cg_rows]\n",
    "\n",
    "    agg_exprs = []\n",
    "    for r in cg_rows:\n",
    "        if r.merge_strategy == \"concat\":\n",
    "            agg_exprs.append(\n",
    "                F.concat_ws(\n",
    "                    r.delimiter or \",\",\n",
    "                    F.sort_array(F.collect_set(r.column_name))\n",
    "                ).alias(r.column_name)\n",
    "            )\n",
    "\n",
    "    pass_through_cols = [\n",
    "        c for c in base_cols if c not in pk_cols and c not in merge_cols\n",
    "    ]\n",
    "\n",
    "    if agg_exprs:\n",
    "        silver_df = (\n",
    "            clean_df\n",
    "            .groupBy(*pk_cols)\n",
    "            .agg(\n",
    "                *agg_exprs,\n",
    "                *[F.first(c, ignorenulls=True).alias(c) for c in pass_through_cols]\n",
    "            )\n",
    "            .withColumn(\"load_timestamp\", F.current_timestamp())\n",
    "        )\n",
    "    else:\n",
    "        silver_df = (\n",
    "            clean_df\n",
    "            .dropDuplicates(pk_cols)\n",
    "            .withColumn(\"load_timestamp\", F.current_timestamp())\n",
    "        )\n",
    "\n",
    "    silver_df.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(silver_table)\n",
    "\n",
    "    silver_cnt = silver_df.count()\n",
    "    business_agg_cnt = clean_pre_agg_cnt - silver_cnt\n",
    "    invalid_cnt = invalid_rows.count()\n",
    "    total_src = df.count()\n",
    "\n",
    "    # =========================================================\n",
    "    # 6Ô∏è‚É£ PRINT BLOCK\n",
    "    # =========================================================\n",
    "    _fmt_line(\"SOURCE\", source_table)\n",
    "    _fmt_line(\"COLUMN MAP\", COLUMN_MAP_TBL)\n",
    "    _fmt_line(\"COLUMN GROUP\", COLUMN_GROUP_TBL)\n",
    "    _fmt_line(\"QUARANTINE\", QUARANTINE_TBL)\n",
    "    _fmt_line(\"SILVER VALID\", silver_table)\n",
    "\n",
    "    print(f\"PK column(s)               : {', '.join(pk_cols)}\")\n",
    "    print(f\"Validation Run ID          : {run_id}\")\n",
    "    print(\"‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\n",
    "    print(f\"Total source rows          : {total_src}\")\n",
    "    print(f\"Quarantined rows           : {invalid_cnt}\")\n",
    "    print(f\"Clean rows before agg      : {clean_pre_agg_cnt}\")\n",
    "    print(f\"Business aggregated rows   : {business_agg_cnt}\")\n",
    "    print(f\"Silver valid rows          : {silver_cnt}\")\n",
    "    print(\"‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\n",
    "    print(f\"Accounting check           : {silver_cnt} + {business_agg_cnt} + {invalid_cnt} = {total_src}\")\n",
    "    print(\"‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\n",
    "    print(f\"‚úÖ Run complete for table: {logical_name}\")\n",
    "\n",
    "    # =========================================================\n",
    "    # 7Ô∏è‚É£ OBSERVABILITY\n",
    "    # =========================================================\n",
    "    metrics = [\n",
    "        (\"Volume\",\"total_raw_records\",str(total_src)),\n",
    "        (\"Volume\",\"total_valid_records\",str(silver_cnt)),\n",
    "        (\"Volume\",\"total_failed_records\",str(invalid_cnt)),\n",
    "        (\"Quality\",\"business_aggregated_records\",str(business_agg_cnt)),\n",
    "        (\"Quality\",\"quarantined_percentage\",\n",
    "         str(round((invalid_cnt * 100.0) / total_src, 2) if total_src else 0))\n",
    "    ]\n",
    "\n",
    "    (\n",
    "        spark.createDataFrame(metrics, [\"metric_category\",\"metric_name\",\"metric_value\"])\n",
    "        .withColumn(\"run_id\", F.lit(run_id))\n",
    "        .withColumn(\"source_table\", F.lit(logical_name))\n",
    "        .withColumn(\"metric_ts\", F.lit(PIPELINE_TS))\n",
    "        .withColumn(\"metric_date\", F.lit(PIPELINE_TS.date()))\n",
    "        .withColumn(\"created_ts\", F.current_timestamp())\n",
    "        .write.mode(\"append\")\n",
    "        .saveAsTable(OBS_TBL)\n",
    "    )\n",
    "\n",
    "# =============================================================\n",
    "# EXECUTION\n",
    "# =============================================================\n",
    "print(f\"üöÄ PIPELINE RUN ID : {PIPELINE_RUN_ID}\")\n",
    "print(f\"‚è±Ô∏è PIPELINE TS     : {PIPELINE_TS}\")\n",
    "\n",
    "for src in tables_list:\n",
    "    run_validation_for_table(src, PIPELINE_RUN_ID)\n",
    "\n",
    "print(\"üéØ VALIDATION + OBSERVABILITY COMPLETED SUCCESSFULLY\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8848650709580992,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Data Quality and Quarantine framework",
   "widgets": {
    "Input_file_location_pk": {
     "currentValue": "pk_file_volumn/pk_files.txt  ",
     "nuid": "dc723141-f2a4-498f-a60f-79340668accc",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "Input_file_location_pk",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "Input_file_location_pk",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "catalog": {
     "currentValue": "main",
     "nuid": "d4078310-d64f-467f-9026-6811016dd171",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "catalog",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "catalog",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "schema": {
     "currentValue": "meta",
     "nuid": "77ba16b8-47df-4619-97ad-8883a36d3b1d",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "schema",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "schema",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "tables": {
     "currentValue": "enrollment,pharmacy_claim",
     "nuid": "c470e0cf-95d3-475e-87a0-3ba4842196b5",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "tables",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "tables",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
