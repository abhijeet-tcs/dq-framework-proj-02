{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aecd5924-8176-40d5-8c0a-ce6e22091e44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Metadata-Driven Ingestion Flow in Databricks\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates a **metadata-driven approach** to ingest data from multiple sources:\n",
    "- **Batch Data**\n",
    "- **Streaming Data**\n",
    "- **API Data**\n",
    "- **JDBC**\n",
    "\n",
    "The ingestion logic is controlled by a parameter (`connection_type`) and metadata stored in a Delta table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a910ed46-1c68-4fac-b665-74ac7ba850dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "selected_connection_type = dbutils.widgets.get(\"connection_type\")\n",
    "print(selected_connection_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7eac730c-d18e-4839-9b9a-75a5adb72a95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE VOLUME IF NOT EXISTS main.meta.parquet_files;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97c2241a-d9c5-4293-b8f5-d7da18fb6df1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run /Users/sarthak.bhatt@tcs.com/Functions_to_write_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "412555c4-0d4b-4b42-9c2b-b56a5b1f5a2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#%run /Workspace/Users/sarthak.bhatt@tcs.com/Functions_to_write_data_cloned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "404aa06c-78ff-4cec-83dd-df4e3ff76bbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b469fca-780b-4189-b34c-51a3695ea25a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "if selected_connection_type == \"batch\":\n",
    "    print(\"Batch ingestion started...\")\n",
    "    batch_config_df = spark.read.table(\"main.meta.batch_config\")\n",
    "    connection_config_df = spark.read.table(\"main.meta.connection_config\")\n",
    "\n",
    "    # filter the enabled as true\n",
    "    file_batches = batch_config_df.filter(batch_config_df.enabled == True)\n",
    "    display(file_batches)\n",
    "    file_batches_with_path = file_batches.join(\n",
    "        connection_config_df,\n",
    "        file_batches.source_id == connection_config_df.source_id,\n",
    "        \"inner\"\n",
    "    )\n",
    "    display(file_batches_with_path)\n",
    "    for row in file_batches_with_path.collect():\n",
    "        source_id = row[\"source_id\"]\n",
    "        source_path = row[\"source_path\"]\n",
    "        file_format = row[\"file_format\"]\n",
    "        target_catalog = row[\"target_catalog\"].split(\".\")[-1]\n",
    "        target_schema = row[\"target_schema\"].split(\".\")[-1]\n",
    "        target_table_name = row[\"target_table\"]\n",
    "        target_file_format = row[\"target_file_format\"] \n",
    "    # Remove any catalog/schema prefix from table name\n",
    "        target_table_name = target_table_name.split(\".\")[-1]\n",
    "        target_table = f\"{target_catalog}.{target_schema}.{target_table_name}\"\n",
    "        print(f\"Processing Batch ID: {source_id} | Source: {source_path} | Target: {target_table}\")\n",
    "        source_df = (\n",
    "            spark.read\n",
    "            .format(file_format)\n",
    "            .option(\"multiline\", \"true\")\n",
    "            .option(\"header\", True)\n",
    "            .option(\"inferSchema\", True)\n",
    "            .load(source_path)\n",
    "        )\n",
    "        display(source_df)\n",
    "        for col in source_df.columns:\n",
    "            clean_col = col.strip().replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"-\", \"_\").replace(\"/\", \"_\")\n",
    "            if clean_col != col:\n",
    "                source_df = source_df.withColumnRenamed(col, clean_col)\n",
    "\n",
    "        write_to_format(source_df, target_table, target_file_format)\n",
    "\n",
    "elif selected_connection_type == \"api\":\n",
    "    import requests\n",
    "    import json\n",
    "    print(\"API ingestion started...\")\n",
    "    api_config_df = spark.read.table(\"main.meta.api_config\")\n",
    "    connection_config_df = spark.read.table(\"main.meta.connection_config\")\n",
    "    api_config_pd = api_config_df.toPandas()\n",
    "    connection_config_pd = connection_config_df.toPandas()\n",
    "    for _, api_row in api_config_pd.iterrows():\n",
    "        source_id = api_row[\"source_id\"]\n",
    "        endpoint = api_row.get(\"endpoint\") or api_row.get(\"api_url\") or \"\"\n",
    "        params = json.loads(api_row.get(\"params\", \"{}\")) if api_row.get(\"params\") else {}\n",
    "        connection_id = api_row.get(\"connection_id\") or api_row.get(\"source_id\")\n",
    "        conn_match = connection_config_pd[connection_config_pd[\"source_id\"] == connection_id]\n",
    "        if conn_match.empty:\n",
    "            print(f\"No connection config for source_id: {connection_id}. Skipping.\")\n",
    "            continue\n",
    "        conn_details = conn_match.iloc[0].to_dict()\n",
    "        base_url = conn_details.get(\"base_url\") or conn_details.get(\"api_url\") or \"\"\n",
    "        auth_token = conn_details.get(\"auth_token\", None)\n",
    "        headers = json.loads(conn_details.get(\"api_headers\", \"{}\")) if conn_details.get(\"api_headers\") else {}\n",
    "        if auth_token:\n",
    "            headers[\"Authorization\"] = f\"Bearer {auth_token}\"\n",
    "        if base_url.endswith(\"/\") and endpoint.startswith(\"/\"):\n",
    "            url = base_url + endpoint[1:]\n",
    "        elif not base_url.endswith(\"/\") and not endpoint.startswith(\"/\"):\n",
    "            url = base_url + \"/\" + endpoint\n",
    "        else:\n",
    "            url = base_url + endpoint\n",
    "\n",
    "     # Dynamically get target catalog, schema, and table\n",
    "\n",
    "        target_catalog = api_row.get(\"target_catalog\", \"main\").split(\".\")[-1]\n",
    "        target_schema = api_row.get(\"target_schema\", \"meta\").split(\".\")[-1]\n",
    "        target_table_name = api_row.get(\"target_table\", \"bronze_api\").split(\".\")[-1]\n",
    "        target_table = f\"{target_catalog}.{target_schema}.{target_table_name}\"\n",
    "        target_file_format = api_row[\"target_file_format\"]\n",
    "        #print(target_file_format)\n",
    "        print(f\"Fetching data from API: {url}\")\n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            if isinstance(data, list):\n",
    "                source_df = spark.createDataFrame(data)\n",
    "            elif isinstance(data, dict) and \"items\" in data:\n",
    "                source_df = spark.createDataFrame(data[\"items\"])\n",
    "            else:\n",
    "                source_df = spark.createDataFrame([data])\n",
    "\n",
    "        write_to_format(source_df, target_table, target_file_format)\n",
    "                \n",
    "        #print(f\"Data from API {source_id} ingested into \" f\"{target_table if target_file_format == 'delta' else f'/Volumes/{target_catalog}/{target_schema}/parquet_files/{source_id}'}\")\n",
    "        \n",
    "    else:\n",
    "            print(f\"Failed to fetch data from API {source_id}. Status: {response.status_code}\")\n",
    "\n",
    "elif selected_connection_type == \"streaming\":\n",
    "    import re\n",
    "    def sanitize_column_name(name):\n",
    "        return re.sub(r'[ ,;{}()\\n\\t=]', '_', name)\n",
    "    print(\"Streaming ingestion started...\")\n",
    "    streaming_config_df = spark.read.table(\"main.meta.streaming_config\")\n",
    "    connection_config_df = spark.read.table(\"main.meta.connection_config\")\n",
    "    streaming_batches = streaming_config_df.join(\n",
    "        connection_config_df,\n",
    "        streaming_config_df.source_id == connection_config_df.source_id,\n",
    "        \"inner\"\n",
    "    )\n",
    "    allowed_formats = [\"csv\", \"parquet\", \"json\", \"avro\"]\n",
    "    for row in streaming_batches.collect():\n",
    "        source_id = row[\"source_id\"]\n",
    "        source_path = row[\"source_path\"]\n",
    "        file_format = row[\"file_format\"]\n",
    "        target_file_format = row[\"target_file_format\"]\n",
    "        # Dynamically construct target table from metadata\n",
    "        target_catalog = row[\"target_catalog\"].split(\".\")[-1]\n",
    "        target_schema = row[\"target_schema\"].split(\".\")[-1]\n",
    "        target_table_name = row[\"target_table\"].split(\".\")[-1]\n",
    "        target_table = f\"{target_catalog}.{target_schema}.{target_table_name}\"\n",
    "        checkpoint_location = f\"/Volumes/{target_catalog}/{target_schema}/checkpoints/{source_id}/checkpoint_v2\"\n",
    "        schema_location = f\"/Volumes/{target_catalog}/{target_schema}/checkpoints/{source_id}/schema_v2\"\n",
    "        df = (\n",
    "            spark.readStream\n",
    "            .format(\"cloudFiles\")\n",
    "            .option(\"cloudFiles.format\", file_format)\n",
    "            .option(\"cloudFiles.schemaLocation\", schema_location)\n",
    "            .option(\"mergeSchema\", \"true\")\n",
    "            .load(source_path)\n",
    "        )\n",
    "        source_df = df.toDF(*[sanitize_column_name(c) for c in df.columns])\n",
    "\n",
    "        if target_file_format == \"delta\":\n",
    "            (\n",
    "                df.writeStream\n",
    "                .format(\"delta\")\n",
    "                .option(\"checkpointLocation\", checkpoint_location)\n",
    "                .option(\"mergeSchema\", \"true\")\n",
    "                .outputMode(\"append\")\n",
    "                .trigger(availableNow=True)\n",
    "                .toTable(target_table)\n",
    "            )\n",
    "        else:\n",
    "            (\n",
    "                df.writeStream\n",
    "                .format(\"parquet\")\n",
    "                .option(\"checkpointLocation\", checkpoint_location)\n",
    "                .outputMode(\"append\")\n",
    "                .trigger(availableNow=True)\n",
    "                .start(f\"/Volumes/{target_catalog}/{target_schema}/parquet_files/{source_id}\")\n",
    "            )\n",
    "\n",
    "elif selected_connection_type == \"jdbc\":\n",
    "    jdbc_config_df = spark.read.table(\"main.meta.jdbc_config\")\n",
    "    connection_config_df = spark.read.table(\"main.meta.connection_config\")\n",
    "    jdbc_batches = jdbc_config_df.join(\n",
    "        connection_config_df,\n",
    "        jdbc_config_df.source_id == connection_config_df.source_id,\n",
    "        \"inner\"\n",
    "    )\n",
    "    for row in jdbc_batches.collect():\n",
    "        jdbc_url = row[\"jdbc_url\"]\n",
    "        jdbc_user = row[\"jdbc_user\"]\n",
    "        jdbc_password = row[\"jdbc_password\"]\n",
    "        # Dynamically construct target table name\n",
    "        target_catalog = row[\"target_catalog\"].split(\".\")[-1]\n",
    "        target_schema = row[\"target_schema\"].split(\".\")[-1]\n",
    "        target_table_name = row[\"target_table\"].split(\".\")[-1]\n",
    "        target_file_format = row[\"target_file_format\"]\n",
    "        target_table = f\"{target_catalog}.{target_schema}.{target_table_name}\"\n",
    "        dbtable = row[\"dbtable\"] if \"dbtable\" in row.asDict() else None\n",
    "        if not dbtable:\n",
    "            print(f\"‚ùå No dbtable specified for source_id {row['source_id']}. Skipping.\")\n",
    "            continue\n",
    "        df = (\n",
    "            spark.read\n",
    "            .format(\"jdbc\")\n",
    "            .option(\"url\", jdbc_url)\n",
    "            .option(\"dbtable\", dbtable)\n",
    "            .option(\"user\", jdbc_user)\n",
    "            .option(\"password\", jdbc_password)\n",
    "            .load()\n",
    "        )\n",
    "        write_to_format(source_df, target_table, target_file_format)\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Invalid source type provided\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c247242-c059-4af6-8fac-9c68ecd0ff3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Checking the count of target table: **main.meta.bronze_target_metadatadriven_ingestion** to ensure data ingestion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2987e430-41e1-46e5-8381-1d9462488347",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from main.meta.bronze_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63eb8ada-0bbc-418f-8645-939fc759a68d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# spark.sql(\"DELETE FROM main.meta.bronze_batch\")\n",
    "# spark.sql(\"DELETE FROM main.meta.bronze_api\")\n",
    "# spark.sql(\"DELETE FROM main.meta.bronze_streaming\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ac32c0e-7496-4d4d-9695-2fb1643e510d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"DELETE FROM main.meta.bronze_batch\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5233955748380007,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "MetadataDriven_Ingestion_Framework",
   "widgets": {
    "connection_type": {
     "currentValue": "batch",
     "nuid": "c9c0c1f2-c060-4d58-b21d-1b49ef492041",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "batch",
      "label": "connection_type",
      "name": "connection_type",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "batch",
        "api",
        "streaming",
        "jdbc"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "batch",
      "label": "connection_type",
      "name": "connection_type",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": false,
       "choices": [
        "batch",
        "api",
        "streaming",
        "jdbc"
       ]
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
